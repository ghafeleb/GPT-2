{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scriptsCPU times: total: 0 ns\n",
      "Wall time: 1min 16s\n",
      "\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cpu\n",
      "model_type: gpt2\n",
      "Loaded model!\n",
      "torch.Size([4, 32, 50257])\n",
      "tensor(10.9620, grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, loss: 11.069896697998047\n",
      "Epoch 2, loss: 6.691137313842773\n",
      "Epoch 3, loss: 4.411059856414795\n",
      "Epoch 4, loss: 2.7389912605285645\n",
      "Epoch 5, loss: 1.5896186828613281\n",
      "Epoch 6, loss: 0.8843767642974854\n",
      "Epoch 7, loss: 0.49447715282440186\n",
      "Epoch 8, loss: 0.28351345658302307\n",
      "Epoch 9, loss: 0.18080806732177734\n",
      "Epoch 10, loss: 0.12094521522521973\n",
      "Epoch 11, loss: 0.08389722555875778\n",
      "Epoch 12, loss: 0.061154358088970184\n",
      "Epoch 13, loss: 0.04657919332385063\n",
      "Epoch 14, loss: 0.03646894171833992\n",
      "Epoch 15, loss: 0.03038286603987217\n",
      "Epoch 16, loss: 0.026429710909724236\n",
      "Epoch 17, loss: 0.023009994998574257\n",
      "Epoch 18, loss: 0.02000425010919571\n",
      "Epoch 19, loss: 0.01768866926431656\n",
      "Epoch 20, loss: 0.0159249696880579\n",
      "Epoch 21, loss: 0.014411654323339462\n",
      "Epoch 22, loss: 0.013000166974961758\n",
      "Epoch 23, loss: 0.011693897657096386\n",
      "Epoch 24, loss: 0.010544849559664726\n",
      "Epoch 25, loss: 0.009583230130374432\n",
      "Epoch 26, loss: 0.008794094435870647\n",
      "Epoch 27, loss: 0.008137768134474754\n",
      "Epoch 28, loss: 0.0075753130950033665\n",
      "Epoch 29, loss: 0.007079276721924543\n",
      "Epoch 30, loss: 0.0066312626004219055\n",
      "Epoch 31, loss: 0.0062224967405200005\n",
      "Epoch 32, loss: 0.005848485045135021\n",
      "Epoch 33, loss: 0.00550730898976326\n",
      "Epoch 34, loss: 0.005198794417083263\n",
      "Epoch 35, loss: 0.004921325948089361\n",
      "Epoch 36, loss: 0.004673060029745102\n",
      "Epoch 37, loss: 0.004450621549040079\n",
      "Epoch 38, loss: 0.0042512016370892525\n",
      "Epoch 39, loss: 0.004071654751896858\n",
      "Epoch 40, loss: 0.003908693324774504\n",
      "Epoch 41, loss: 0.003760161343961954\n",
      "Epoch 42, loss: 0.003623847384005785\n",
      "Epoch 43, loss: 0.0034983258228749037\n",
      "Epoch 44, loss: 0.0033822942059487104\n",
      "Epoch 45, loss: 0.003274790942668915\n",
      "Epoch 46, loss: 0.0031751280184835196\n",
      "Epoch 47, loss: 0.0030825056601315737\n",
      "Epoch 48, loss: 0.002996702678501606\n",
      "Epoch 49, loss: 0.002916984260082245\n",
      "Epoch 50, loss: 0.0028430938255041838\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2_test.py --train --data_type super_tiny_shakespear --lr 3e-4 --optimizer adam --epochs 50 --device cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scriptsCPU times: total: 0 ns\n",
      "Wall time: 18 s\n",
      "\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "Loaded model!\n",
      "torch.Size([4, 32, 50257])\n",
      "tensor(10.9044, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
      "Epoch 1, loss: 10.797444343566895\n",
      "Epoch 2, loss: 8.212468147277832\n",
      "Epoch 3, loss: 7.389337062835693\n",
      "Epoch 4, loss: 14.134679794311523\n",
      "Epoch 5, loss: 9.839411735534668\n",
      "Epoch 6, loss: 9.718757629394531\n",
      "Epoch 7, loss: 7.567324638366699\n",
      "Epoch 8, loss: 7.265936374664307\n",
      "Epoch 9, loss: 7.108125686645508\n",
      "Epoch 10, loss: 6.968127250671387\n",
      "Epoch 11, loss: 6.801562309265137\n",
      "Epoch 12, loss: 6.583417892456055\n",
      "Epoch 13, loss: 6.325179576873779\n",
      "Epoch 14, loss: 6.058863639831543\n",
      "Epoch 15, loss: 5.807836532592773\n",
      "Epoch 16, loss: 5.577911853790283\n",
      "Epoch 17, loss: 5.3657708168029785\n",
      "Epoch 18, loss: 5.1638360023498535\n",
      "Epoch 19, loss: 4.9609832763671875\n",
      "Epoch 20, loss: 4.7524495124816895\n",
      "Epoch 21, loss: 4.547922611236572\n",
      "Epoch 22, loss: 4.3596696853637695\n",
      "Epoch 23, loss: 4.188434600830078\n",
      "Epoch 24, loss: 4.025635242462158\n",
      "Epoch 25, loss: 3.870269536972046\n",
      "Epoch 26, loss: 3.7275495529174805\n",
      "Epoch 27, loss: 3.573620319366455\n",
      "Epoch 28, loss: 3.4446871280670166\n",
      "Epoch 29, loss: 3.3341052532196045\n",
      "Epoch 30, loss: 3.225757598876953\n",
      "Epoch 31, loss: 3.1114768981933594\n",
      "Epoch 32, loss: 2.986755132675171\n",
      "Epoch 33, loss: 2.8670620918273926\n",
      "Epoch 34, loss: 2.755969285964966\n",
      "Epoch 35, loss: 2.695065498352051\n",
      "Epoch 36, loss: 2.648385524749756\n",
      "Epoch 37, loss: 2.792483329772949\n",
      "Epoch 38, loss: 2.7881009578704834\n",
      "Epoch 39, loss: 2.5653507709503174\n",
      "Epoch 40, loss: 2.421224355697632\n",
      "Epoch 41, loss: 2.3981497287750244\n",
      "Epoch 42, loss: 2.273942470550537\n",
      "Epoch 43, loss: 2.1156063079833984\n",
      "Epoch 44, loss: 2.042266607284546\n",
      "Epoch 45, loss: 1.949561595916748\n",
      "Epoch 46, loss: 1.8003616333007812\n",
      "Epoch 47, loss: 1.712831974029541\n",
      "Epoch 48, loss: 1.6749062538146973\n",
      "Epoch 49, loss: 1.5987941026687622\n",
      "Epoch 50, loss: 1.4898290634155273\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2_test.py --train --data_type super_tiny_shakespear --lr 3e-4 --optimizer adam --epochs 50 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "Epoch 1, loss: 10.924736022949219, Run time: 4215.00 ms\n",
      "Epoch 2, loss: 9.452760696411133, Run time: 204.00 ms\n",
      "Epoch 3, loss: 9.091828346252441, Run time: 195.00 ms\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 19.7 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 3 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "Epoch 1, loss: 10.924736022949219, Run time: 1650.97 ms, token/sec: 620.2426945107454\n",
      "Epoch 2, loss: 9.452760696411133, Run time: 202.00 ms, token/sec: 5069.345271663501\n",
      "Epoch 3, loss: 9.091828346252441, Run time: 193.00 ms, token/sec: 5305.716994977134\n",
      "Epoch 4, loss: 8.757145881652832, Run time: 193.00 ms, token/sec: 5305.703886349598\n",
      "Epoch 5, loss: 8.872180938720703, Run time: 193.00 ms, token/sec: 5305.690777786836\n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 5 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "50257\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "Epoch 1, loss: 10.924736976623535, Run time: 1788.97 ms, token/sec: 572.3974006163281\n",
      "Epoch 2, loss: 9.452759742736816, Run time: 189.00 ms, token/sec: 5417.95152694048\n",
      "Epoch 3, loss: 9.091829299926758, Run time: 188.00 ms, token/sec: 5446.788837992624\n",
      "Epoch 4, loss: 8.757147789001465, Run time: 187.00 ms, token/sec: 5475.969676029223\n",
      "Epoch 5, loss: 8.872180938720703, Run time: 184.00 ms, token/sec: 5565.1808742907415\n",
      "Epoch 6, loss: 8.355034828186035, Run time: 185.00 ms, token/sec: 5535.158247448589\n",
      "Epoch 7, loss: 8.180399894714355, Run time: 184.00 ms, token/sec: 5565.260197007824\n",
      "Epoch 8, loss: 7.992529392242432, Run time: 184.00 ms, token/sec: 5565.224140947015\n",
      "Epoch 9, loss: 7.933725833892822, Run time: 184.00 ms, token/sec: 5565.202507534785\n",
      "Epoch 10, loss: 7.699762344360352, Run time: 184.03 ms, token/sec: 5564.214760892406\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 12 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "50304\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "Epoch 1, loss: 10.929630279541016, Run time: 1744.01 ms, token/sec: 587.1515626757965\n",
      "Epoch 2, loss: 9.539490699768066, Run time: 193.00 ms, token/sec: 5305.723549315192\n",
      "Epoch 3, loss: 9.116418838500977, Run time: 184.00 ms, token/sec: 5565.209718653507\n",
      "Epoch 4, loss: 8.868069648742676, Run time: 185.00 ms, token/sec: 5535.115447020358\n",
      "Epoch 5, loss: 8.896907806396484, Run time: 185.00 ms, token/sec: 5535.122580379098\n",
      "Epoch 6, loss: 8.341182708740234, Run time: 181.97 ms, token/sec: 5627.422851310433\n",
      "Epoch 7, loss: 8.250682830810547, Run time: 184.03 ms, token/sec: 5564.337308939023\n",
      "Epoch 8, loss: 8.036262512207031, Run time: 181.97 ms, token/sec: 5627.290135855217\n",
      "Epoch 9, loss: 7.955794334411621, Run time: 186.00 ms, token/sec: 5505.33078509563\n",
      "Epoch 10, loss: 7.733007907867432, Run time: 187.03 ms, token/sec: 5474.9993893958845\n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 11.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "\n",
      "Epoch 1 | loss: 10.929630 | Run time: 1677.00 ms | token/sec: 610.61 \n",
      "Epoch 2 | loss: 9.539492 | Run time: 180.03 ms | token/sec: 5687.86 \n",
      "Epoch 3 | loss: 9.117859 | Run time: 172.97 ms | token/sec: 5920.22 \n",
      "Epoch 4 | loss: 8.870733 | Run time: 176.00 ms | token/sec: 5818.19 \n",
      "Epoch 5 | loss: 8.900723 | Run time: 173.00 ms | token/sec: 5919.07 \n",
      "Epoch 6 | loss: 8.346569 | Run time: 174.00 ms | token/sec: 5885.03 \n",
      "Epoch 7 | loss: 8.256947 | Run time: 174.00 ms | token/sec: 5885.08 \n",
      "Epoch 8 | loss: 8.044099 | Run time: 171.00 ms | token/sec: 5988.31 \n",
      "Epoch 9 | loss: 7.964668 | Run time: 174.03 ms | token/sec: 5883.90 \n",
      "Epoch 10 | loss: 7.741519 | Run time: 172.97 ms | token/sec: 5920.26 \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --gpt3_adam_beta --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "\n",
      "Epoch 1 | loss: 10.929630 | Run time: 1706.00 ms | token/sec: 600.23  | norm: 32.5467 \n",
      "Epoch 2 | loss: 9.540201 | Run time: 185.97 ms | token/sec: 5506.33  | norm: 8.0450 \n",
      "Epoch 3 | loss: 9.096671 | Run time: 177.00 ms | token/sec: 5785.33  | norm: 2.5119 \n",
      "Epoch 4 | loss: 9.105254 | Run time: 178.04 ms | token/sec: 5751.61  | norm: 5.2657 \n",
      "Epoch 5 | loss: 8.951261 | Run time: 177.96 ms | token/sec: 5753.94  | norm: 3.5531 \n",
      "Epoch 6 | loss: 8.340922 | Run time: 177.03 ms | token/sec: 5784.32  | norm: 2.9244 \n",
      "Epoch 7 | loss: 8.315553 | Run time: 177.00 ms | token/sec: 5785.22  | norm: 3.6758 \n",
      "Epoch 8 | loss: 8.080706 | Run time: 177.97 ms | token/sec: 5753.88  | norm: 2.7443 \n",
      "Epoch 9 | loss: 7.980969 | Run time: 179.00 ms | token/sec: 5720.67  | norm: 1.7628 \n",
      "Epoch 10 | loss: 7.741008 | Run time: 180.03 ms | token/sec: 5687.85  | norm: 2.2829 \n",
      "CPU times: total: 0 ns\n",
      "Wall time: 11.1 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --clip_grad_norm --gpt3_adam_beta --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "\n",
      "Epoch 1 | loss: 10.929630 | Run time: 1897.05 ms | token/sec: 539.78  | norm: 32.5467  | lr: 6.0000e-05 \n",
      "Epoch 2 | loss: 9.642627 | Run time: 195.03 ms | token/sec: 5250.42  | norm: 12.1801  | lr: 1.2000e-04 \n",
      "Epoch 3 | loss: 9.232815 | Run time: 182.00 ms | token/sec: 5626.43  | norm: 3.9833  | lr: 1.8000e-04 \n",
      "Epoch 4 | loss: 9.229612 | Run time: 181.00 ms | token/sec: 5657.35  | norm: 6.8409  | lr: 2.4000e-04 \n",
      "Epoch 5 | loss: 9.046021 | Run time: 180.00 ms | token/sec: 5688.81  | norm: 2.5938  | lr: 3.0000e-04 \n",
      "Epoch 6 | loss: 8.477405 | Run time: 183.00 ms | token/sec: 5595.74  | norm: 3.1550  | lr: 3.6000e-04 \n",
      "Epoch 7 | loss: 8.407369 | Run time: 182.00 ms | token/sec: 5626.34  | norm: 3.0815  | lr: 4.2000e-04 \n",
      "Epoch 8 | loss: 8.211511 | Run time: 183.03 ms | token/sec: 5594.72  | norm: 3.2672  | lr: 4.8000e-04 \n",
      "Epoch 9 | loss: 7.959211 | Run time: 185.02 ms | token/sec: 5534.56  | norm: 1.8535  | lr: 5.4000e-04 \n",
      "Epoch 10 | loss: 7.576886 | Run time: 185.95 ms | token/sec: 5506.72  | norm: 3.4395  | lr: 6.0000e-04 \n",
      "Epoch 11 | loss: 6.988237 | Run time: 185.00 ms | token/sec: 5535.07  | norm: 1.8676  | lr: 6.0000e-04 \n",
      "Epoch 12 | loss: 7.312613 | Run time: 182.99 ms | token/sec: 5596.09  | norm: 1.7807  | lr: 5.9917e-04 \n",
      "Epoch 13 | loss: 6.635297 | Run time: 181.01 ms | token/sec: 5657.21  | norm: 2.2992  | lr: 5.9668e-04 \n",
      "Epoch 14 | loss: 6.854315 | Run time: 181.99 ms | token/sec: 5626.63  | norm: 1.7309  | lr: 5.9254e-04 \n",
      "Epoch 15 | loss: 6.566823 | Run time: 184.02 ms | token/sec: 5564.50  | norm: 1.6719  | lr: 5.8679e-04 \n",
      "Epoch 16 | loss: 6.513342 | Run time: 182.00 ms | token/sec: 5626.41  | norm: 1.2950  | lr: 5.7945e-04 \n",
      "Epoch 17 | loss: 6.328729 | Run time: 182.00 ms | token/sec: 5626.38  | norm: 1.3793  | lr: 5.7057e-04 \n",
      "Epoch 18 | loss: 6.308811 | Run time: 181.00 ms | token/sec: 5657.43  | norm: 1.7730  | lr: 5.6021e-04 \n",
      "Epoch 19 | loss: 8.014440 | Run time: 184.00 ms | token/sec: 5565.20  | norm: 16.1695  | lr: 5.4843e-04 \n",
      "Epoch 20 | loss: 6.395974 | Run time: 186.00 ms | token/sec: 5505.42  | norm: 1.7335  | lr: 5.3531e-04 \n",
      "Epoch 21 | loss: 5.838889 | Run time: 187.01 ms | token/sec: 5475.64  | norm: 1.8141  | lr: 5.2092e-04 \n",
      "Epoch 22 | loss: 6.609316 | Run time: 182.99 ms | token/sec: 5595.92  | norm: 1.5389  | lr: 5.0535e-04 \n",
      "Epoch 23 | loss: 5.890191 | Run time: 186.00 ms | token/sec: 5505.39  | norm: 1.6360  | lr: 4.8870e-04 \n",
      "Epoch 24 | loss: 5.612039 | Run time: 189.02 ms | token/sec: 5417.34  | norm: 1.8429  | lr: 4.7107e-04 \n",
      "Epoch 25 | loss: 6.052021 | Run time: 189.98 ms | token/sec: 5390.07  | norm: 1.5082  | lr: 4.5258e-04 \n",
      "Epoch 26 | loss: 6.047064 | Run time: 192.03 ms | token/sec: 5332.38  | norm: 1.5928  | lr: 4.3332e-04 \n",
      "Epoch 27 | loss: 6.058149 | Run time: 191.01 ms | token/sec: 5361.05  | norm: 1.2439  | lr: 4.1343e-04 \n",
      "Epoch 28 | loss: 5.722631 | Run time: 192.02 ms | token/sec: 5332.74  | norm: 1.2250  | lr: 3.9303e-04 \n",
      "Epoch 29 | loss: 5.961638 | Run time: 187.00 ms | token/sec: 5476.04  | norm: 1.4097  | lr: 3.7224e-04 \n",
      "Epoch 30 | loss: 5.968343 | Run time: 189.03 ms | token/sec: 5417.05  | norm: 1.1795  | lr: 3.5118e-04 \n",
      "Epoch 31 | loss: 5.959182 | Run time: 193.00 ms | token/sec: 5305.72  | norm: 1.4577  | lr: 3.3000e-04 \n",
      "Epoch 32 | loss: 5.912569 | Run time: 194.00 ms | token/sec: 5278.34  | norm: 2.2069  | lr: 3.0882e-04 \n",
      "Epoch 33 | loss: 6.320231 | Run time: 190.00 ms | token/sec: 5389.44  | norm: 1.9929  | lr: 2.8776e-04 \n",
      "Epoch 34 | loss: 6.323586 | Run time: 185.97 ms | token/sec: 5506.27  | norm: 1.7339  | lr: 2.6697e-04 \n",
      "Epoch 35 | loss: 6.159887 | Run time: 187.00 ms | token/sec: 5476.03  | norm: 1.5913  | lr: 2.4657e-04 \n",
      "Epoch 36 | loss: 5.678178 | Run time: 192.00 ms | token/sec: 5333.32  | norm: 2.2332  | lr: 2.2668e-04 \n",
      "Epoch 37 | loss: 5.677614 | Run time: 193.00 ms | token/sec: 5305.66  | norm: 1.7007  | lr: 2.0742e-04 \n",
      "Epoch 38 | loss: 6.100632 | Run time: 191.00 ms | token/sec: 5361.30  | norm: 1.2704  | lr: 1.8893e-04 \n",
      "Epoch 39 | loss: 5.540086 | Run time: 187.00 ms | token/sec: 5475.85  | norm: 1.5786  | lr: 1.7130e-04 \n",
      "Epoch 40 | loss: 6.251873 | Run time: 189.03 ms | token/sec: 5417.13  | norm: 1.7613  | lr: 1.5465e-04 \n",
      "Epoch 41 | loss: 6.172918 | Run time: 187.96 ms | token/sec: 5447.84  | norm: 1.2000  | lr: 1.3908e-04 \n",
      "Epoch 42 | loss: 6.254882 | Run time: 185.00 ms | token/sec: 5535.12  | norm: 1.2487  | lr: 1.2469e-04 \n",
      "Epoch 43 | loss: 6.101077 | Run time: 188.00 ms | token/sec: 5446.82  | norm: 1.1019  | lr: 1.1157e-04 \n",
      "Epoch 44 | loss: 6.120900 | Run time: 190.04 ms | token/sec: 5388.44  | norm: 1.2990  | lr: 9.9787e-05 \n",
      "Epoch 45 | loss: 6.145618 | Run time: 187.02 ms | token/sec: 5475.22  | norm: 1.4860  | lr: 8.9428e-05 \n",
      "Epoch 46 | loss: 6.079785 | Run time: 193.00 ms | token/sec: 5305.69  | norm: 1.5529  | lr: 8.0553e-05 \n",
      "Epoch 47 | loss: 6.966261 | Run time: 187.00 ms | token/sec: 5476.00  | norm: 1.3167  | lr: 7.3215e-05 \n",
      "Epoch 48 | loss: 6.725560 | Run time: 183.97 ms | token/sec: 5566.07  | norm: 1.6538  | lr: 6.7460e-05 \n",
      "Epoch 49 | loss: 6.980815 | Run time: 191.03 ms | token/sec: 5360.44  | norm: 1.5415  | lr: 6.3324e-05 \n",
      "Epoch 50 | loss: 6.866630 | Run time: 180.97 ms | token/sec: 5658.38  | norm: 2.3372  | lr: 6.0832e-05 \n",
      "CPU times: total: 0 ns\n",
      "Wall time: 20 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --lr_scheduler \"cosine\" --clip_grad_norm --gpt3_adam_beta --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 50 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "\n",
      "Epoch 1 | loss: 10.929630 | Run time: 1771.97 ms | token/sec: 577.89  | norm: 32.5467  | lr: 6.0000e-05 \n",
      "Epoch 2 | loss: 9.642627 | Run time: 198.00 ms | token/sec: 5171.74  | norm: 12.1801  | lr: 1.2000e-04 \n",
      "Epoch 3 | loss: 9.232353 | Run time: 187.00 ms | token/sec: 5476.00  | norm: 3.9675  | lr: 1.8000e-04 \n",
      "Epoch 4 | loss: 9.218202 | Run time: 185.54 ms | token/sec: 5519.06  | norm: 6.7785  | lr: 2.4000e-04 \n",
      "Epoch 5 | loss: 9.043883 | Run time: 186.00 ms | token/sec: 5505.52  | norm: 2.5392  | lr: 3.0000e-04 \n",
      "Epoch 6 | loss: 8.446630 | Run time: 187.98 ms | token/sec: 5447.37  | norm: 2.8198  | lr: 3.6000e-04 \n",
      "Epoch 7 | loss: 8.385722 | Run time: 188.02 ms | token/sec: 5446.24  | norm: 2.9956  | lr: 4.2000e-04 \n",
      "Epoch 8 | loss: 8.171364 | Run time: 187.00 ms | token/sec: 5475.86  | norm: 3.0306  | lr: 4.8000e-04 \n",
      "Epoch 9 | loss: 7.926305 | Run time: 187.99 ms | token/sec: 5447.04  | norm: 1.8858  | lr: 5.4000e-04 \n",
      "Epoch 10 | loss: 7.490327 | Run time: 185.01 ms | token/sec: 5534.95  | norm: 2.1141  | lr: 6.0000e-04 \n",
      "Epoch 11 | loss: 6.931733 | Run time: 187.97 ms | token/sec: 5447.62  | norm: 1.8667  | lr: 6.0000e-04 \n",
      "Epoch 12 | loss: 7.243807 | Run time: 187.00 ms | token/sec: 5476.06  | norm: 1.5423  | lr: 5.9917e-04 \n",
      "Epoch 13 | loss: 6.566211 | Run time: 190.04 ms | token/sec: 5388.42  | norm: 1.8063  | lr: 5.9668e-04 \n",
      "Epoch 14 | loss: 6.833184 | Run time: 186.97 ms | token/sec: 5476.92  | norm: 2.0718  | lr: 5.9254e-04 \n",
      "Epoch 15 | loss: 6.539433 | Run time: 183.00 ms | token/sec: 5595.62  | norm: 1.5410  | lr: 5.8679e-04 \n",
      "Epoch 16 | loss: 6.537044 | Run time: 186.01 ms | token/sec: 5505.20  | norm: 2.8737  | lr: 5.7945e-04 \n",
      "Epoch 17 | loss: 6.334036 | Run time: 185.99 ms | token/sec: 5505.53  | norm: 1.5978  | lr: 5.7057e-04 \n",
      "Epoch 18 | loss: 6.363482 | Run time: 187.00 ms | token/sec: 5475.93  | norm: 2.4090  | lr: 5.6021e-04 \n",
      "Epoch 19 | loss: 6.189977 | Run time: 186.00 ms | token/sec: 5505.47  | norm: 3.0790  | lr: 5.4843e-04 \n",
      "Epoch 20 | loss: 6.386377 | Run time: 182.03 ms | token/sec: 5625.55  | norm: 1.4871  | lr: 5.3531e-04 \n",
      "Epoch 21 | loss: 5.765858 | Run time: 183.01 ms | token/sec: 5595.46  | norm: 1.6240  | lr: 5.2092e-04 \n",
      "Epoch 22 | loss: 6.505592 | Run time: 184.00 ms | token/sec: 5565.09  | norm: 1.4925  | lr: 5.0535e-04 \n",
      "Epoch 23 | loss: 5.866387 | Run time: 184.00 ms | token/sec: 5565.36  | norm: 3.1529  | lr: 4.8870e-04 \n",
      "Epoch 24 | loss: 5.536516 | Run time: 182.97 ms | token/sec: 5596.51  | norm: 2.0383  | lr: 4.7107e-04 \n",
      "Epoch 25 | loss: 6.047521 | Run time: 183.03 ms | token/sec: 5594.74  | norm: 1.8069  | lr: 4.5258e-04 \n",
      "Epoch 26 | loss: 6.011588 | Run time: 185.00 ms | token/sec: 5535.15  | norm: 1.4982  | lr: 4.3332e-04 \n",
      "Epoch 27 | loss: 6.051570 | Run time: 186.01 ms | token/sec: 5505.10  | norm: 1.2279  | lr: 4.1343e-04 \n",
      "Epoch 28 | loss: 5.709335 | Run time: 186.97 ms | token/sec: 5476.91  | norm: 1.1286  | lr: 3.9303e-04 \n",
      "Epoch 29 | loss: 5.918288 | Run time: 181.02 ms | token/sec: 5656.96  | norm: 1.2033  | lr: 3.7224e-04 \n",
      "Epoch 30 | loss: 5.945832 | Run time: 184.02 ms | token/sec: 5564.74  | norm: 1.3252  | lr: 3.5118e-04 \n",
      "Epoch 31 | loss: 5.923973 | Run time: 183.98 ms | token/sec: 5565.84  | norm: 1.3659  | lr: 3.3000e-04 \n",
      "Epoch 32 | loss: 5.845212 | Run time: 192.00 ms | token/sec: 5333.39  | norm: 1.6708  | lr: 3.0882e-04 \n",
      "Epoch 33 | loss: 6.282423 | Run time: 188.03 ms | token/sec: 5445.82  | norm: 1.8955  | lr: 2.8776e-04 \n",
      "Epoch 34 | loss: 6.365696 | Run time: 185.97 ms | token/sec: 5506.29  | norm: 2.7251  | lr: 2.6697e-04 \n",
      "Epoch 35 | loss: 6.200125 | Run time: 182.00 ms | token/sec: 5626.52  | norm: 2.6707  | lr: 2.4657e-04 \n",
      "Epoch 36 | loss: 5.639824 | Run time: 186.03 ms | token/sec: 5504.39  | norm: 2.2387  | lr: 2.2668e-04 \n",
      "Epoch 37 | loss: 5.637074 | Run time: 182.52 ms | token/sec: 5610.38  | norm: 1.8667  | lr: 2.0742e-04 \n",
      "Epoch 38 | loss: 6.090734 | Run time: 181.00 ms | token/sec: 5657.52  | norm: 2.1066  | lr: 1.8893e-04 \n",
      "Epoch 39 | loss: 5.509352 | Run time: 182.02 ms | token/sec: 5625.65  | norm: 1.5862  | lr: 1.7130e-04 \n",
      "Epoch 40 | loss: 6.213402 | Run time: 183.97 ms | token/sec: 5566.20  | norm: 1.4778  | lr: 1.5465e-04 \n",
      "Epoch 41 | loss: 6.145114 | Run time: 183.00 ms | token/sec: 5595.67  | norm: 1.3525  | lr: 1.3908e-04 \n",
      "Epoch 42 | loss: 6.235680 | Run time: 185.01 ms | token/sec: 5534.97  | norm: 1.5933  | lr: 1.2469e-04 \n",
      "Epoch 43 | loss: 6.079095 | Run time: 186.99 ms | token/sec: 5476.13  | norm: 1.1892  | lr: 1.1157e-04 \n",
      "Epoch 44 | loss: 6.120682 | Run time: 185.03 ms | token/sec: 5534.30  | norm: 1.5063  | lr: 9.9787e-05 \n",
      "Epoch 45 | loss: 6.138013 | Run time: 185.00 ms | token/sec: 5535.16  | norm: 1.5373  | lr: 8.9428e-05 \n",
      "Epoch 46 | loss: 6.063576 | Run time: 184.01 ms | token/sec: 5565.03  | norm: 1.5239  | lr: 8.0553e-05 \n",
      "Epoch 47 | loss: 6.957572 | Run time: 187.00 ms | token/sec: 5475.93  | norm: 1.2319  | lr: 7.3215e-05 \n",
      "Epoch 48 | loss: 6.696131 | Run time: 190.97 ms | token/sec: 5362.14  | norm: 1.6046  | lr: 6.7460e-05 \n",
      "Epoch 49 | loss: 6.971087 | Run time: 189.00 ms | token/sec: 5418.02  | norm: 1.5416  | lr: 6.3324e-05 \n",
      "Epoch 50 | loss: 6.827768 | Run time: 191.04 ms | token/sec: 5360.07  | norm: 2.1954  | lr: 6.0832e-05 \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 19.6 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:40: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --lr_scheduler \"cosine\" --clip_grad_norm --gpt3_adam_parameters --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 50 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "\n",
      "Epoch 1 | loss: 10.929630 | Run time: 1646.00 ms | token/sec: 622.11  | norm: 32.5467  | lr: 6.0000e-05 \n",
      "Epoch 2 | loss: 9.642629 | Run time: 169.00 ms | token/sec: 6059.23  | norm: 12.1802  | lr: 1.2000e-04 \n",
      "Epoch 3 | loss: 9.232823 | Run time: 160.00 ms | token/sec: 6399.97  | norm: 3.9832  | lr: 1.8000e-04 \n",
      "Epoch 4 | loss: 9.229555 | Run time: 161.00 ms | token/sec: 6360.19  | norm: 6.8405  | lr: 2.4000e-04 \n",
      "Epoch 5 | loss: 9.046054 | Run time: 158.00 ms | token/sec: 6481.11  | norm: 2.5936  | lr: 3.0000e-04 \n",
      "Epoch 6 | loss: 8.477379 | Run time: 159.00 ms | token/sec: 6440.25  | norm: 3.1530  | lr: 3.6000e-04 \n",
      "Epoch 7 | loss: 8.407454 | Run time: 159.00 ms | token/sec: 6440.25  | norm: 3.0819  | lr: 4.2000e-04 \n",
      "Epoch 8 | loss: 8.211561 | Run time: 161.00 ms | token/sec: 6360.20  | norm: 3.2669  | lr: 4.8000e-04 \n",
      "Epoch 9 | loss: 7.959291 | Run time: 160.00 ms | token/sec: 6400.03  | norm: 1.8528  | lr: 5.4000e-04 \n",
      "Epoch 10 | loss: 7.576871 | Run time: 163.00 ms | token/sec: 6282.20  | norm: 3.4364  | lr: 6.0000e-04 \n",
      "CPU times: total: 0 ns\n",
      "Wall time: 10.9 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:41: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --total_batch_size -1 --lr_scheduler \"cosine\" --clip_grad_norm --gpt3_adam_parameters --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\n",
      "Running on cuda\n",
      "model_type: gpt2\n",
      "args.compile_model: False\n",
      "Loaded model!\n",
      "num decayed parameter tensors: 50, with 124,354,560 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "Loaded 338025 tokens\n",
      "1 epoch = 330 batches\n",
      "total desired batch size: 2048\n",
      "=> calculated gradient accumulation steps: 2\n",
      "\n",
      "Epoch 1 | loss: 5.454886 | Run time: 1804.00 ms | token/sec: 1135.26  | norm: 16.1286  | lr: 6.0000e-05 \n",
      "Epoch 2 | loss: 4.790714 | Run time: 310.00 ms | token/sec: 6606.46  | norm: 6.5714  | lr: 1.2000e-04 \n",
      "Epoch 3 | loss: 4.682109 | Run time: 304.00 ms | token/sec: 6736.84  | norm: 7.4270  | lr: 1.8000e-04 \n",
      "Epoch 4 | loss: 4.803279 | Run time: 302.00 ms | token/sec: 6781.46  | norm: 3.2455  | lr: 2.4000e-04 \n",
      "Epoch 5 | loss: 4.537816 | Run time: 307.00 ms | token/sec: 6671.01  | norm: 2.0913  | lr: 3.0000e-04 \n",
      "Epoch 6 | loss: 4.475208 | Run time: 305.00 ms | token/sec: 6714.75  | norm: 1.3622  | lr: 3.6000e-04 \n",
      "Epoch 7 | loss: 4.267072 | Run time: 304.00 ms | token/sec: 6736.82  | norm: 1.3247  | lr: 4.2000e-04 \n",
      "Epoch 8 | loss: 4.123302 | Run time: 301.00 ms | token/sec: 6804.01  | norm: 1.0925  | lr: 4.8000e-04 \n",
      "Epoch 9 | loss: 3.915634 | Run time: 300.00 ms | token/sec: 6826.68  | norm: 1.5083  | lr: 5.4000e-04 \n",
      "Epoch 10 | loss: 3.779154 | Run time: 303.00 ms | token/sec: 6759.09  | norm: 1.3882  | lr: 6.0000e-04 \n",
      "CPU times: total: 15.6 ms\n",
      "Wall time: 12.4 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Ali\\anaconda3\\envs\\llm\\lib\\site-packages\\torch\\__init__.py:1073: UserWarning: 'high' is not one of 'highest', 'high', or 'medium'; the currentsetFloat32MatmulPrecision call has no effect. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\Context.cpp:244.)\n",
      "  _C._set_float32_matmul_precision(precision)\n",
      "c:\\Users\\Ali\\Desktop\\gpt-2\\scripts\\..\\model\\gpt2.py:41: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:455.)\n",
      "  y = F.scaled_dot_product_attention(q, k, v, is_causal=True)\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "!python ../train/train_gpt2.py --total_batch_size 2048 --lr_scheduler \"cosine\" --clip_grad_norm --gpt3_adam_parameters --vocab_size 50304 --flash_attention --autocast_type 'bf16' --matmul_precision 'high' --batch_size 1 --token_size 1024 --train --data_type tiny_shakespear --lr 3e-4 --optimizer adam --epochs 10 --device cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
